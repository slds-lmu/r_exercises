---
title: "mlr3 - key concepts"
output:
  learnr::tutorial:
    progressive: yes
    allow_skip : false
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
set.seed(123)
library(learnr)
library(rchallenge)
library(mlr3)
library(mlr3learners)
library(skimr)
library(mlr3viz)

source("test_functions.R", local = knitr::knit_global())

tutorial_options(
  exercise.checker = checker_endpoint,
  exercise.reveal_solution = FALSE,
  exercise.completion = FALSE
)
```

## 1 Data {data-progressive=FALSE}
In this tutorial, we want to give you an idea of how you can use mlr3 to build your own applied machine learning project. mlr3 is an open-source collection of packages providing a unified interface for machine learning in R.
Throughout the exercise, we will use two well-known data sets: "Boston housing", a regression problem, and "German credit", a classification problem. In general, we will use "Boston housing" as an example and ask you to implement something similar for "German credit".

### 1.1 Boston housing
[Boston housing](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) contains information on housing for 506 census tracts of the city of Boston from the 1970 census. In this use case, we want to build a model that predicts the median value (in USD 1,000s) of owner-occupied homes in each census tract (medv). This is a regression problem.
```{r}
data("BostonHousing", package = "mlbench")
skim(BostonHousing)
```

### 1.2 German credit
For the [German credit](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) data, we are interested in predicting a person's credit risk (good or bad), by using information contained in 20 features describing an individual's personal, demographic and financial status. This is a binary classification problem.
```{r}
data("german", package = "rchallenge")
skim(german)
```


## 2 Tasks
In mlr3, we usually start by specifying *what* should be modeled. This is called "task". A task describes the type of machine learning problem, with the most common ones being regression and classification. Furthermore, the task contains the data, the target variable, and a few other elements.

### 2.1 Defining a task
In order to define a task with mlr3, we need to create an object: for regression problems, this is an object of class "TaskRegr", for classification problems, an object of class "TaskClassif". There are several ways to define a task in mlr3. If we wanted to instantiat the task for our regression problem, we could use the following code:  
```{r}
taskRegr = as_task_regr(x = BostonHousing, id = "BostonHousing", target = "medv")
```

In a similar manner, we want to define a task for our classification problem. Create a new classification task for the German credit problem!  

<div id="task-formal">
**Note:** You can proceed in a way similar to the regression task, but there is more than one option to do this with mlr3. The only thing that matters is that you instantiate an object of class "TaskClassif" with the correct data backend and target variable. Take a look into the mlr3 documentation of tasks with '?mlr3::Task'
</div>

To test your code click "Submit Answer". To run your code click "Run Code".

```{r task, exercise=TRUE}
taskClassif = "your code"
```

```{r task-check}
checker_endpoint()
```

```{r task-hint-1}
# Concerning the data backend and target variable, take a look at section 1.1
```

```{r task-hint-2}
taskClassif = as_task_classif("your code")
```

This is your code playground. You can use this chunk to take a further look at the objects that we have instantiated.
```{r pg1, exercise=TRUE}

```

### 2.2 Questions
```{r quiz_task, echo=FALSE}
question("Which of the following statements are true?",
  answer("Tasks objects contain the data backend.", correct = TRUE),
  answer("When we instantiate a task, we have to specify the learner we want to solve it with."),
  answer("Tasks store meta-information about our data (e.g., the names and types of the feature variables in the data)", correct = TRUE),
  allow_retry = TRUE
)
``` 

### 2.3 Creating a train/test split 
Often, it is not a good idea to evaluate a model's performance on the same observations that we use for training the model. In mlr3, we can use the partition() function to assign the ids of a task object to either the train or the test set. Here, we use 80% of the observations for training and 20% for testing (selected randomly):
```{r}
splitsRegr <-  mlr3::partition(taskRegr, ratio = 0.8)
```

Now it's your turn. Split the ids of the backend of taskClassif into ids for training and testing using a ratio of 0.8. 

<div id="split-hint">
**Note:** You don't have to use the partition()-function! 
The only thing that matters is that you have a named list (names: "train " and "test") consisting of two integer vectors which contain the corresponding ids of the train/test observations.
</div>

```{r data, exercise=TRUE, exercise.reveal_solution = FALSE}
splitsClassif <- "your code"
```

```{r data-check}
checker_endpoint()
```


## 3 Learner
After we have defined *what* should be modeled with a task, we can now specify *how*. The actual learning is done via the so called "learner" class.

### 3.1 Defining a learner
In he code chunk below, we have already defined our learner for the regression problem. Here, we use "regr.rpart" to specify that we want to fit a regression tree with the "rpart" package.
```{r}
lrnRegr = lrn("regr.rpart")
```

However, for our classification task, we want to train a random forest, specifically, a random forest trained with the "ranger" package. Create a new random forest learner for the German credit classification problem!

```{r learner, exercise=TRUE}
lrnClassif = "Your code"
```

```{r learner-check}
checker_endpoint()
```

<div id="filter-hint">
**Hint:** Have a look at the learners available in mlr3 and find the one for "ranger": https://mlr3learners.mlr-org.com/.
</div>

Another code playground:
```{r pg2, exercise=TRUE}

```

### 3.2 Training a model
Now it's time to use our learner to train a model! Again, we have already trained the regression model on the corresponding task, using only the observations reserved for training in our train/test split:

```{r}
lrnRegr$train(taskRegr, row_ids = splitsRegr$train)
```

Now it's your turn to use the classification learner we have defined in the previously to fit a model on the corresponding task! For training, make sure to only use the observations you have assigned to the training set.

```{r train, exercise=TRUE}
"your code"
```

```{r train-check}
checker_endpoint()
```

```{r train-solution}
lrnClassif$train(taskClassif, row_ids = splitsClassif$train)
```

### 3.3 A look into the fitted model
Calling train() on a learner produces a fitted model. This model is stored within the learner object itself and can be accessed via the "$model"-field of a learner.
Lets' have a look into our random forest:
```{r pg3, exercise = TRUE}
print(lrnClassif$model)
```
Look into the information of the fitted classification model and check the correct answer: 
```{r quiz_model, echo=FALSE}
question("What is the out-of-bag (OOB) prediction error of the fitted random forest?",
  answer("23.5%", correct = TRUE),
  answer("12.0%"),
  answer("35.2%"),
  answer("4.0%")
)
```

### 3.4 Prediction
Now it's time to make some predictions using our test data. This can be done through the $predict() function of a (trained) learner, which returns an object of class "Prediction". If we print this object, it returns a data.table containing information on the row ids, the true values and predicted response variable.

```{r}
predRegr <- lrnRegr$predict(taskRegr, row_ids = splitsRegr$test)
print(predRegr)
```

Generate predictions for the classification problem using the random forest model trained before! Use the test data defined previously!

```{r predict, exercise=TRUE}
predClassif <- "your code"
```

```{r predict-check}
checker_endpoint()
```

```{r predict-solution}
predClassif <- lrnClassif$predict(taskClassif, row_ids = splitsClassif$test)
```

## 4 Performance evaluation
To estimate the performance of a model on new unseen data, we can use the test data to compare our model's predictions with the true values (holdout splitting). Later in the tutorial, we will consider other (more refined) methods for performance evaluation. 

In mlr3, we can use the "$score()"-function to retrieve performance measures from a "Prediction" object. For the regression problem, we could use the mean squared error (MSE) to assess performance.

The corresponding MSE is:
```{r}
predRegr$score(measures = msr("regr.mse"))
```

Similarly, evaluate the performance of our classification model with respect to the classification error. 
```{r pe, exercise=TRUE}
"your code"
```

```{r pe-check}
checker_endpoint()
```

```{r pe-solution}
predClassif$score(measures = msr("classif.ce"))
```

